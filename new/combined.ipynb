{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_model",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlM0HjNIe78O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "171e8344-b443-40d2-9958-47ae64d1553f"
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import math\n",
        "import os\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, Conv2D, Reshape, MaxPooling2D\n",
        "from keras.layers import Concatenate, Lambda, Dot, BatchNormalization, Flatten"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.14`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFGEyXalic57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# functions to execute dominant sets clustering algorithm\n",
        "# http://homepage.tudelft.nl/3e2t5/HungKrose_ICMI2011.pdf\n",
        "\n",
        "# fills an n_people x n_people matrix with affinity values.\n",
        "def learned_affinity(preds, n_people):\n",
        "\tA = np.zeros((n_people, n_people))\n",
        "\tidx = 0\n",
        "\n",
        "\tfor i in range(n_people):\n",
        "\t\tfor j in range(n_people):\n",
        "\t\t\tif i == j: continue\n",
        "\t\t\tA[i,j] += preds[idx]/2\n",
        "\t\t\tA[j,i] += preds[idx]/2\n",
        "\t\t\tidx += 1\n",
        "\n",
        "\treturn A\n",
        "\n",
        "# d-sets function k\n",
        "def k(S, i, A):\n",
        "\tsum_affs = 0\n",
        "\tfor j in range(len(S)):\n",
        "\t\tif S[j]: sum_affs += A[i,j]\n",
        "\n",
        "\treturn 1/np.sum(S) * sum_affs\n",
        "\n",
        "# d-sets function phi\n",
        "def phi(S,i,j,A):\n",
        "\treturn A[i,j] - k(S,i,A)\n",
        "\n",
        "# d-sets function weight\n",
        "def weight(S, i, A):\n",
        "\tif np.sum(S) == 1:\n",
        "\t\treturn 1\n",
        "\telse:\n",
        "\t\tR = S.copy()\n",
        "\t\tR[i] = False\n",
        "\t\tsum_weights = 0\n",
        "\t\tfor j in range(len(R)):\n",
        "\t\t\tif R[j]:\n",
        "\t\t\t\tsum_weights += phi(R,j,i,A) * weight(R,j,A)\n",
        "\t\t\t\treturn sum_weights\n",
        "\n",
        "## optimization function\n",
        "def f(x, A):\n",
        "\treturn np.dot(x.T, np.dot(A, x))\n",
        "\n",
        "## iteratively finds vector x which maximizes f\n",
        "def vector_climb(A, allowed, n_people, thres=1e-5):\n",
        "\tx = np.random.uniform(0,1,n_people)\n",
        "\tx = np.multiply(x, allowed)\n",
        "\teps = 10\n",
        "\tn = 10\n",
        "\n",
        "\twhile (eps > 1e-15):\n",
        "\t\tp = f(x,A)\n",
        "\t\tx = np.multiply(x, np.dot(A,x)) / np.dot(x, np.dot(A,x))\n",
        "\t\tn = f(x,A)\n",
        "\t\teps = abs(n-p)\n",
        "\n",
        "\tgroups = x > thres\n",
        "\n",
        "\tfor i in range(n_people):\n",
        "\t\tif not allowed[i]:\n",
        "\t\t\tif weight(groups,i,A) > 0.0:\n",
        "\t\t\t\treturn []\n",
        "\n",
        "\treturn groups\n",
        "\n",
        "def process_groups(bool_groups, n_people):\n",
        "    groups = []\n",
        "    for bool_group in bool_groups:\n",
        "        group = []\n",
        "        for i in range(n_people):\n",
        "            if (bool_group[i]): group.append(\"ID_00\" + str(i+1))\n",
        "        if(len(group)>1): groups.append(group)\n",
        "    return groups\n",
        "\n",
        "# Finds vectors x of people which maximize f. Then removes those people and repeats\n",
        "# main method\n",
        "def ds(preds, n_people):\n",
        "    allowed = np.ones(n_people)\n",
        "    groups = []\n",
        "\n",
        "    A = learned_affinity(preds, n_people)\n",
        "\n",
        "    while (np.sum(allowed) > 1):\n",
        "        A[allowed == False] = 0\n",
        "        A[:,allowed == False] = 0\n",
        "\n",
        "        if (np.sum(np.dot(allowed,A)) == 0): break\n",
        "        x = vector_climb(A, allowed, n_people, thres=1e-5)\n",
        "        if len(x) == 0: break\n",
        "        groups.append(x)\n",
        "\n",
        "        allowed = np.multiply(x == False, allowed)\n",
        "\n",
        "    return process_groups(groups, n_people)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvJohHPOifgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_f1(X, Y, times, preds, thres):\n",
        "    results = np.array([0.0, 0.0])\n",
        "    for i in range(len(times)-1):\n",
        "        start, stop = int(times[i]), int(times[i+1])\n",
        "        num_people = int(np.sqrt(stop-start))+1\n",
        "\n",
        "        pred = ds(preds[start:stop], num_people)\n",
        "        truth = ds(Y[start:stop], num_people)\n",
        "\n",
        "        TF, FN, FP, P, R = indiv_f1(pred, truth, thres)\n",
        "        results += np.array([P, R])\n",
        "\n",
        "    results /= (len(times) - 1)\n",
        "    P, R = results\n",
        "\n",
        "    f1 = 2 * P * R / (P + R)\n",
        "    return P, R, f1\n",
        "\n",
        "## calculates true positives, false negatives, and false positives\n",
        "## given the guesses, the true groups, and the threshold T\n",
        "def indiv_f1(pred, truth, T):\n",
        "    TP, FN, FP = 0, 0, 0\n",
        "    n_true_groups = len(truth)\n",
        "    n_pred_groups = len(pred)\n",
        "\n",
        "    for true_group in truth:\n",
        "        for pred_group in pred:\n",
        "            n_found = 0\n",
        "            for person in pred_group:\n",
        "                if person in true_group:\n",
        "                    n_found += 1\n",
        "\n",
        "            n_total = max(len(true_group), len(pred_group))\n",
        "            acc = float(n_found) / n_total\n",
        "            if acc>=T: TP += 1\n",
        "\n",
        "    if n_true_groups == 0 and n_pred_groups == 0:\n",
        "        return [0, 0, 0, 1, 1]\n",
        "    elif n_true_groups == 0:\n",
        "        return [0, n_pred_groups, 0, 0, 1]\n",
        "    elif n_pred_groups == 0:\n",
        "        return [0, 0, n_true_groups, 1, 0]\n",
        "    else:\n",
        "        FP = n_pred_groups - TP\n",
        "        FN = n_true_groups - TP\n",
        "        P = float(TP) / (TP + FP) #precision\n",
        "        R = float(TP) / (TP + FN) #recall\n",
        "        return TP, FN, FP, P, R"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqboERMLiiEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "'''\n",
        "standard feature set\n",
        "downloaded = drive.CreateFile({'id':\"1d7EKeFXGlcKA_9XplIkGrIRQS-eGM8_w\"})   # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('train.p')        # replace the file name with your file\n",
        "\n",
        "downloaded = drive.CreateFile({'id':\"18rLSsWTWybeSs0r0S73CoiitgcLH3xZJ\"})   # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('test.p')        # replace the file name with your file\n",
        "\n",
        "downloaded = drive.CreateFile({'id':\"1pmkvRbnl7zvgb6scazItMITlp2GyMQlL\"})   # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('val.p')        # replace the file name with your file\n",
        "'''\n",
        "\n",
        "downloaded = drive.CreateFile({'id':\"1xhR_ptuiC1mprelvE2Sfb4BDcwMAlTy0\"})   # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('train.p')        # replace the file name with your file\n",
        "\n",
        "downloaded = drive.CreateFile({'id':\"1wsFIqrosQx4CHFbH476rQz-WBLJqRG_K\"})   # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('test.p')        # replace the file name with your file\n",
        "\n",
        "downloaded = drive.CreateFile({'id':\"1RtYY820FZTmHvkC6VSNIiZRWi7XW8Yc1\"})   # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('val.p')        # replace the file name with your file\n",
        "\n",
        "def load_matrix(file):\n",
        "    with open(file, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def load_data(dataset):\n",
        "    path = \"./datasets/\" + str(dataset) + \"/processed\"\n",
        "    train = load_matrix('train.p')\n",
        "    test = load_matrix('test.p')\n",
        "    val = load_matrix('val.p')\n",
        "    return train, test, val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZQDrw4SfI4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ValLoss(keras.callbacks.Callback):\n",
        "\n",
        "    def __init__(self, val):\n",
        "        super(ValLoss, self).__init__()\n",
        "        self.X, self.Y, self.times = val\n",
        "\n",
        "        self.best_model = None\n",
        "        self.best_val_mse = float(\"inf\")\n",
        "        self.best_epoch = -1\n",
        "\n",
        "        self.val_f1 = {\"f1s\": [], \"best_f1\": float('-inf')}\n",
        "        self.val_f2_3 = {\"f1s\": [], \"best_f1\": float('-inf')}\n",
        "\n",
        "        self.val_losses = []\n",
        "        self.train_losses = []\n",
        "\n",
        "        self.val_mses = []\n",
        "        self.train_mses = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        print(logs)\n",
        "        if logs['val_mean_squared_error'] < self.best_val_mse:\n",
        "            self.best_model = self.model\n",
        "            self.best_val_mse = logs['val_mean_squared_error']\n",
        "            self.best_epoch = epoch\n",
        "\n",
        "        f_1 = calc_f1(self.X, self.Y, self.times, self.model.predict(self.X), 1)[2]\n",
        "        f_2_3 = calc_f1(self.X, self.Y, self.times, self.model.predict(self.X), 2/3)[2]\n",
        "\n",
        "        for f_1, obj in [(f_1, self.val_f1), (f_2_3, self.val_f2_3)]:\n",
        "            if f_1 > obj['best_f1']:\n",
        "                obj['best_f1'] = f_1\n",
        "                obj['epoch'] = epoch\n",
        "                obj['model'] = self.model\n",
        "            obj['f1s'].append(f_1)\n",
        "\n",
        "        self.val_losses.append(logs['val_loss'])\n",
        "        self.train_losses.append(logs['loss'])\n",
        "        self.val_mses.append(logs['val_mean_squared_error'])\n",
        "        self.train_mses.append(logs['mean_squared_error'])\n",
        "\n",
        "def write_history(file_name, history, test):\n",
        "    file = open(file_name, 'w+')\n",
        "\n",
        "    file.write(\"best_val: \" + str(history.best_val_mse))\n",
        "    file.write(\"\\nepoch: \" + str(history.best_epoch))\n",
        "\n",
        "    file.write(\"\\nbest_val_f1_1: \" + str(history.val_f1['best_f1']))\n",
        "    file.write(\"\\nepoch: \" + str(history.val_f1['epoch']))\n",
        "\n",
        "    X_test, Y_test, times_test = test\n",
        "    preds = history.val_f1['model'].predict(X_test)\n",
        "    p23, r23, f23 = calc_f1(X_test, Y_test, times_test, preds, 2/3)\n",
        "    p1, r1, f1 = calc_f1(X_test, Y_test, times_test, preds, 1)\n",
        "\n",
        "    file.write(\"\\ntest_f1s: \" + str(f23) + \" \" + str(f1))\n",
        "    file.write('\\nprecisions: ' + str(p23) + \" \" + str(p1))\n",
        "    file.write('\\nrecalls: ' + str(r23) + \" \" + str(r1))\n",
        "\n",
        "    file.write(\"\\nbest_val_f1_2/3: \" + str(history.val_f2_3['best_f1']))\n",
        "    file.write(\"\\nepoch: \" + str(history.val_f2_3['epoch']))\n",
        "\n",
        "    X_test, Y_test, times_test = test\n",
        "    preds = history.val_f2_3['model'].predict(X_test)\n",
        "    p23, r23, f23 = calc_f1(X_test, Y_test, times_test, preds, 2/3)\n",
        "    p1, r1, f1 = calc_f1(X_test, Y_test, times_test, preds, 1)\n",
        "\n",
        "    file.write(\"\\ntest_f1s: \" + str(f23) + \" \" + str(f1))\n",
        "    file.write('\\nprecisions: ' + str(p23) + \" \" + str(p1))\n",
        "    file.write('\\nrecalls: ' + str(r23) + \" \" + str(r1))\n",
        "\n",
        "    file.write(\"\\ntrain loss:\")\n",
        "    for loss in history.train_losses:\n",
        "        file.write('\\n' + str(loss))\n",
        "\n",
        "    file.write(\"\\nval loss:\")\n",
        "    for loss in history.val_losses:\n",
        "        file.write('\\n' + str(loss))\n",
        "\n",
        "    file.write(\"\\ntrain mse:\")\n",
        "    for loss in history.train_mses:\n",
        "        file.write('\\n' + str(loss))\n",
        "\n",
        "    file.write(\"\\nval mse:\")\n",
        "    for loss in history.val_mses:\n",
        "        file.write('\\n' + str(loss))\n",
        "\n",
        "    file.write(\"\\nval 1 f1:\")\n",
        "    for f1 in history.val_f1['f1s']:\n",
        "        file.write('\\n' + str(f1))\n",
        "\n",
        "    file.write(\"\\nval 2/3 f1:\")\n",
        "    for f1 in history.val_f2_3['f1s']:\n",
        "        file.write('\\n' + str(f1))\n",
        "\n",
        "    file.close()\n",
        "\n",
        "def conv(filters, reg, name=None):\n",
        "    return Conv2D(filters=filters, kernel_size=1, padding='valid', kernel_initializer=\"he_normal\",\n",
        "        use_bias='True', kernel_regularizer=reg, activation=tf.nn.relu, name=name)\n",
        "\n",
        "def build_model(reg_amt, drop_amt, max_people, num_features, global_filters, individual_filters, combined_filters):\n",
        "\n",
        "    group_inputs = keras.layers.Input(shape=(1, max_people, num_features))\n",
        "    pair_inputs = keras.layers.Input(shape=(1, 2, num_features))\n",
        "\n",
        "    reg = keras.regularizers.l2(reg_amt)\n",
        "\n",
        "    y = pair_inputs\n",
        "\n",
        "    # Dyad Transform\n",
        "    for filters in individual_filters:\n",
        "        y = conv(filters, reg)(y)\n",
        "        y = Dropout(drop_amt)(y)\n",
        "        y = BatchNormalization()(y)\n",
        "\n",
        "    y_0 = Lambda(lambda input: tf.slice(input, [0, 0, 0, 0], [-1, -1, 1, -1]))(y)\n",
        "    y_1 = Lambda(lambda input: tf.slice(input, [0, 0, 1, 0], [-1, -1, 1, -1]))(y)\n",
        "\n",
        "    x = group_inputs\n",
        "\n",
        "    # Context Transform\n",
        "    for filters in global_filters:\n",
        "        x = conv(filters, reg)(x)\n",
        "        x = Dropout(drop_amt)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "    x = MaxPooling2D(name=\"global_pool\", pool_size=[1, max_people], strides=1, padding='valid')(x)\n",
        "    x = Dropout(drop_amt)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x_flat = Flatten()(x)\n",
        "\n",
        "    concat = Concatenate(name='concat')([x_flat, Flatten()(y_0), Flatten()(y_1)])\n",
        "\n",
        "    # Final MLP from paper\n",
        "    for filters in combined_filters:\n",
        "        concat = Dense(units=filters, use_bias='True', kernel_regularizer=reg, activation=tf.nn.relu,\n",
        "            kernel_initializer=\"he_normal\")(concat)\n",
        "        concat = Dropout(drop_amt)(concat)\n",
        "        concat = BatchNormalization()(concat)\n",
        "\n",
        "    # final pred\n",
        "    affinity = Dense(units=1, use_bias=\"True\", kernel_regularizer=reg, activation=tf.nn.sigmoid,\n",
        "        name='affinity', kernel_initializer=\"glorot_normal\")(concat)\n",
        "\n",
        "    model = Model(inputs=[group_inputs, pair_inputs], outputs=affinity)\n",
        "\n",
        "    opt = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, decay=1e-5, amsgrad=False, clipvalue=0.5)\n",
        "    model.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=['mean_squared_error'])\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LBwTVfGgAES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# constructs a model, trains it with early stopping based on validation loss, and then\n",
        "# saves the output to a .txt file.\n",
        "def train_and_save_model(global_filters, individual_filters, combined_filters,\n",
        "    train, val, test, model_path, epochs=600, reg=0.0000001, dropout=.35):\n",
        "\n",
        "    # ensures repeatability\n",
        "    tf.set_random_seed(0)\n",
        "    np.random.seed(0)\n",
        "\n",
        "    num_train, _, max_people, num_features = train[0][0].shape\n",
        "\n",
        "    # save achitecture\n",
        "    if not os.path.isdir(model_path): os.makedirs(model_path)\n",
        "    file = open(model_path + '/architecture.txt', 'w+')\n",
        "    file.write(\"global: \" + str(global_filters) + \"\\nindividual: \" +\n",
        "        str(individual_filters) + \"\\ncombined: \" + str(combined_filters) +\n",
        "        \"\\nreg= \" + str(reg) + \"\\ndropout= \" + str(dropout))\n",
        "\n",
        "    best_val_mses = []\n",
        "    best_val_f1 = []\n",
        "    best_val_f2_3 = []\n",
        "\n",
        "    X_train, Y_train, times_train = train\n",
        "    X_test, Y_test, times_test = test\n",
        "    X_val, Y_val, times_val = val\n",
        "\n",
        "    # build model\n",
        "    model = build_model(reg, dropout, max_people, num_features,\n",
        "        global_filters, individual_filters, combined_filters)\n",
        "\n",
        "    # train model val_mse\n",
        "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
        "    history = ValLoss(test) #custom callback implemented above\n",
        "\n",
        "    #os.system('cls') #hides annoying warnings\n",
        "    model.fit(X_train, Y_train, epochs=epochs, batch_size=1024,\n",
        "        validation_data=(X_val, Y_val), callbacks=[history, early_stop])\n",
        "\n",
        "    best_val_mses.append(history.best_val_mse)\n",
        "    best_val_f1.append(history.val_f1['best_f1'])\n",
        "    best_val_f2_3.append(history.val_f2_3['best_f1'])\n",
        "\n",
        "    # save model\n",
        "    write_history(model_path + '/results.txt', history, test)\n",
        "    history.val_f1['model'].save(model_path + '/model.h5')\n",
        "\n",
        "    file.write(\"\\n\\nbest overall val loss: \" + str(min(best_val_mses)))\n",
        "    file.write(\"\\n\\nbest overall f1 1: \" + str(max(best_val_f1)))\n",
        "    file.write(\"\\n\\nbest overall f1 2/3: \" + str(max(best_val_f2_3)))\n",
        "    file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2KMFVtAgDw3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "64159c8a-d1bf-44e3-a2e1-f6bc97cb825c"
      },
      "source": [
        "global_filters = [16, 128, 512]\n",
        "individual_filters = [32]\n",
        "combined_filters = [1024, 256, 256]\n",
        "epochs = 600\n",
        "reg = 7.943282347242822e-05\n",
        "dropout = 0.15\n",
        "\n",
        "train, test, val = load_data(\"cocktail\")\n",
        "model_path = \"./models/cocktail_expanded/model2\"\n",
        "\n",
        "train_and_save_model(global_filters, individual_filters, combined_filters,\n",
        "    train, val, test, model_path, epochs, reg, dropout)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 13440 samples, validate on 1920 samples\n",
            "Epoch 1/600\n",
            "13440/13440 [==============================] - 4s 309us/step - loss: 1.1897 - mean_squared_error: 0.2916 - val_loss: 1.1476 - val_mean_squared_error: 0.2780\n",
            "{'val_loss': 1.1476022283236185, 'val_mean_squared_error': 0.2779996673266093, 'loss': 1.1897113345918202, 'mean_squared_error': 0.29157915541103907}\n",
            "Epoch 2/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 1.0993 - mean_squared_error: 0.2572 - val_loss: 1.0510 - val_mean_squared_error: 0.2448\n",
            "{'val_loss': 1.050971007347107, 'val_mean_squared_error': 0.2448214148481687, 'loss': 1.099290867078872, 'mean_squared_error': 0.25716616426195416}\n",
            "Epoch 3/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 1.0387 - mean_squared_error: 0.2347 - val_loss: 0.9835 - val_mean_squared_error: 0.2203\n",
            "{'val_loss': 0.9835008025169373, 'val_mean_squared_error': 0.22030828098456065, 'loss': 1.0386580069859823, 'mean_squared_error': 0.23467345436414083}\n",
            "Epoch 4/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 1.0074 - mean_squared_error: 0.2219 - val_loss: 0.9626 - val_mean_squared_error: 0.2118\n",
            "{'val_loss': 0.9626374324162801, 'val_mean_squared_error': 0.2118288531899452, 'loss': 1.0074439582370576, 'mean_squared_error': 0.22187623424189432}\n",
            "Epoch 5/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.9778 - mean_squared_error: 0.2096 - val_loss: 0.9666 - val_mean_squared_error: 0.2124\n",
            "{'val_loss': 0.9665561079978943, 'val_mean_squared_error': 0.2123707483212153, 'loss': 0.9777822011993045, 'mean_squared_error': 0.2096486680564426}\n",
            "Epoch 6/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.9586 - mean_squared_error: 0.2026 - val_loss: 0.9192 - val_mean_squared_error: 0.1950\n",
            "{'val_loss': 0.9192150672276814, 'val_mean_squared_error': 0.1950024977326393, 'loss': 0.9585916099094209, 'mean_squared_error': 0.20258503896849497}\n",
            "Epoch 7/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.9375 - mean_squared_error: 0.1941 - val_loss: 0.8833 - val_mean_squared_error: 0.1814\n",
            "{'val_loss': 0.8832698027292888, 'val_mean_squared_error': 0.18136465400457383, 'loss': 0.937543219044095, 'mean_squared_error': 0.19412919311296373}\n",
            "Epoch 8/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.9342 - mean_squared_error: 0.1910 - val_loss: 0.8659 - val_mean_squared_error: 0.1746\n",
            "{'val_loss': 0.8658811251322428, 'val_mean_squared_error': 0.17455771913131077, 'loss': 0.9342264283271063, 'mean_squared_error': 0.1910251851592745}\n",
            "Epoch 9/600\n",
            "13440/13440 [==============================] - 0s 32us/step - loss: 0.9033 - mean_squared_error: 0.1800 - val_loss: 0.8648 - val_mean_squared_error: 0.1737\n",
            "{'val_loss': 0.8648494561513265, 'val_mean_squared_error': 0.17371322611967724, 'loss': 0.903323305220831, 'mean_squared_error': 0.18000295531182062}\n",
            "Epoch 10/600\n",
            "13440/13440 [==============================] - 0s 32us/step - loss: 0.8882 - mean_squared_error: 0.1748 - val_loss: 0.8937 - val_mean_squared_error: 0.1825\n",
            "{'val_loss': 0.8936681350072225, 'val_mean_squared_error': 0.18253970940907796, 'loss': 0.8882217838650658, 'mean_squared_error': 0.17476141637279874}\n",
            "Epoch 11/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.8718 - mean_squared_error: 0.1674 - val_loss: 0.9097 - val_mean_squared_error: 0.1866\n",
            "{'val_loss': 0.9097110509872437, 'val_mean_squared_error': 0.18656713316837947, 'loss': 0.871791664759318, 'mean_squared_error': 0.16735608279705047}\n",
            "Epoch 12/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.8661 - mean_squared_error: 0.1657 - val_loss: 0.8753 - val_mean_squared_error: 0.1745\n",
            "{'val_loss': 0.8753001530965169, 'val_mean_squared_error': 0.17453072766462963, 'loss': 0.8661011888867333, 'mean_squared_error': 0.16571420161497025}\n",
            "Epoch 13/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.8540 - mean_squared_error: 0.1609 - val_loss: 0.8247 - val_mean_squared_error: 0.1569\n",
            "{'val_loss': 0.8247454444567363, 'val_mean_squared_error': 0.15687178472677868, 'loss': 0.8539999479339236, 'mean_squared_error': 0.16087556665851957}\n",
            "Epoch 14/600\n",
            "13440/13440 [==============================] - 0s 30us/step - loss: 0.8485 - mean_squared_error: 0.1592 - val_loss: 0.8163 - val_mean_squared_error: 0.1534\n",
            "{'val_loss': 0.8162506937980651, 'val_mean_squared_error': 0.15341680496931076, 'loss': 0.8485425591468811, 'mean_squared_error': 0.15919124881426494}\n",
            "Epoch 15/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.8282 - mean_squared_error: 0.1508 - val_loss: 0.8054 - val_mean_squared_error: 0.1498\n",
            "{'val_loss': 0.8053890188535054, 'val_mean_squared_error': 0.1498475228746732, 'loss': 0.8281683955873762, 'mean_squared_error': 0.1507513057617914}\n",
            "Epoch 16/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.8261 - mean_squared_error: 0.1492 - val_loss: 0.8128 - val_mean_squared_error: 0.1521\n",
            "{'val_loss': 0.8127546946207682, 'val_mean_squared_error': 0.15211836546659468, 'loss': 0.8260897415024894, 'mean_squared_error': 0.14923436123700368}\n",
            "Epoch 17/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.8090 - mean_squared_error: 0.1430 - val_loss: 0.8417 - val_mean_squared_error: 0.1603\n",
            "{'val_loss': 0.8417308370272318, 'val_mean_squared_error': 0.16032893508672713, 'loss': 0.8089547043754941, 'mean_squared_error': 0.14301842465287162}\n",
            "Epoch 18/600\n",
            "13440/13440 [==============================] - 0s 30us/step - loss: 0.8000 - mean_squared_error: 0.1386 - val_loss: 0.7805 - val_mean_squared_error: 0.1411\n",
            "{'val_loss': 0.7805295507113139, 'val_mean_squared_error': 0.14107049976785976, 'loss': 0.8000206180981227, 'mean_squared_error': 0.1386330367553802}\n",
            "Epoch 19/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.7925 - mean_squared_error: 0.1368 - val_loss: 0.7530 - val_mean_squared_error: 0.1317\n",
            "{'val_loss': 0.7530358036359152, 'val_mean_squared_error': 0.1317356901864211, 'loss': 0.7924738742056348, 'mean_squared_error': 0.13675714830557506}\n",
            "Epoch 20/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.7877 - mean_squared_error: 0.1349 - val_loss: 0.7584 - val_mean_squared_error: 0.1334\n",
            "{'val_loss': 0.7583531061808269, 'val_mean_squared_error': 0.13341091747085254, 'loss': 0.7877443035443624, 'mean_squared_error': 0.1348792791366577}\n",
            "Epoch 21/600\n",
            "13440/13440 [==============================] - 0s 30us/step - loss: 0.7786 - mean_squared_error: 0.1303 - val_loss: 0.7311 - val_mean_squared_error: 0.1243\n",
            "{'val_loss': 0.7311331391334533, 'val_mean_squared_error': 0.12432895277937253, 'loss': 0.7785652762367612, 'mean_squared_error': 0.13028784947735922}\n",
            "Epoch 22/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.7730 - mean_squared_error: 0.1290 - val_loss: 0.7497 - val_mean_squared_error: 0.1300\n",
            "{'val_loss': 0.7496662298838298, 'val_mean_squared_error': 0.12999564955631893, 'loss': 0.7730383203143165, 'mean_squared_error': 0.12903618358430408}\n",
            "Epoch 23/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.7639 - mean_squared_error: 0.1259 - val_loss: 0.6969 - val_mean_squared_error: 0.1123\n",
            "{'val_loss': 0.6968539555867513, 'val_mean_squared_error': 0.11228239859143893, 'loss': 0.7638638212567284, 'mean_squared_error': 0.12586162296079453}\n",
            "Epoch 24/600\n",
            "13440/13440 [==============================] - 0s 30us/step - loss: 0.7667 - mean_squared_error: 0.1254 - val_loss: 0.7050 - val_mean_squared_error: 0.1151\n",
            "{'val_loss': 0.7050059715906779, 'val_mean_squared_error': 0.11507086604833602, 'loss': 0.7666814963022868, 'mean_squared_error': 0.1254273303917476}\n",
            "Epoch 25/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.7614 - mean_squared_error: 0.1245 - val_loss: 0.7303 - val_mean_squared_error: 0.1232\n",
            "{'val_loss': 0.7303068518638611, 'val_mean_squared_error': 0.12320574099818865, 'loss': 0.7613679295494443, 'mean_squared_error': 0.12453885234537579}\n",
            "Epoch 26/600\n",
            "13440/13440 [==============================] - 0s 32us/step - loss: 0.7472 - mean_squared_error: 0.1198 - val_loss: 0.7243 - val_mean_squared_error: 0.1210\n",
            "{'val_loss': 0.7242578029632568, 'val_mean_squared_error': 0.12098638266324997, 'loss': 0.7472138279960269, 'mean_squared_error': 0.1197737238236836}\n",
            "Epoch 27/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.7453 - mean_squared_error: 0.1191 - val_loss: 0.7230 - val_mean_squared_error: 0.1205\n",
            "{'val_loss': 0.7229750315348308, 'val_mean_squared_error': 0.12051250711083412, 'loss': 0.7453092035793123, 'mean_squared_error': 0.119087553024292}\n",
            "Epoch 28/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.7404 - mean_squared_error: 0.1165 - val_loss: 0.7311 - val_mean_squared_error: 0.1228\n",
            "{'val_loss': 0.7310723304748535, 'val_mean_squared_error': 0.12278967847426732, 'loss': 0.7403954534303574, 'mean_squared_error': 0.11648624397459484}\n",
            "Epoch 29/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.7361 - mean_squared_error: 0.1158 - val_loss: 0.7225 - val_mean_squared_error: 0.1197\n",
            "{'val_loss': 0.7225212057431539, 'val_mean_squared_error': 0.11972221781810125, 'loss': 0.736129421279544, 'mean_squared_error': 0.11578125684034257}\n",
            "Epoch 30/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.7337 - mean_squared_error: 0.1145 - val_loss: 0.6742 - val_mean_squared_error: 0.1043\n",
            "{'val_loss': 0.6742296814918518, 'val_mean_squared_error': 0.10431306983033815, 'loss': 0.7337260110037668, 'mean_squared_error': 0.11445658788794563}\n",
            "Epoch 31/600\n",
            "13440/13440 [==============================] - 0s 30us/step - loss: 0.7292 - mean_squared_error: 0.1134 - val_loss: 0.6784 - val_mean_squared_error: 0.1056\n",
            "{'val_loss': 0.6783629695574442, 'val_mean_squared_error': 0.10559322312474251, 'loss': 0.7291569840340387, 'mean_squared_error': 0.11339794915346872}\n",
            "Epoch 32/600\n",
            "13440/13440 [==============================] - 0s 30us/step - loss: 0.7245 - mean_squared_error: 0.1106 - val_loss: 0.7088 - val_mean_squared_error: 0.1151\n",
            "{'val_loss': 0.7087904334068298, 'val_mean_squared_error': 0.11505360081791878, 'loss': 0.7244834610394069, 'mean_squared_error': 0.1105606483561652}\n",
            "Epoch 33/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.7255 - mean_squared_error: 0.1130 - val_loss: 0.7571 - val_mean_squared_error: 0.1296\n",
            "{'val_loss': 0.7570908745129903, 'val_mean_squared_error': 0.12957000384728115, 'loss': 0.7255136461485, 'mean_squared_error': 0.11303307655311766}\n",
            "Epoch 34/600\n",
            "13440/13440 [==============================] - 0s 32us/step - loss: 0.7236 - mean_squared_error: 0.1116 - val_loss: 0.7305 - val_mean_squared_error: 0.1217\n",
            "{'val_loss': 0.7305182774861654, 'val_mean_squared_error': 0.12169711962342263, 'loss': 0.7236147749991644, 'mean_squared_error': 0.11164902931167967}\n",
            "Epoch 35/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.7150 - mean_squared_error: 0.1085 - val_loss: 0.7234 - val_mean_squared_error: 0.1196\n",
            "{'val_loss': 0.7233886639277141, 'val_mean_squared_error': 0.11960982829332352, 'loss': 0.7150130408150809, 'mean_squared_error': 0.10847267593656268}\n",
            "Epoch 36/600\n",
            "13440/13440 [==============================] - 0s 32us/step - loss: 0.7156 - mean_squared_error: 0.1091 - val_loss: 0.7364 - val_mean_squared_error: 0.1239\n",
            "{'val_loss': 0.7364263256390889, 'val_mean_squared_error': 0.12388513634602229, 'loss': 0.7155808017367409, 'mean_squared_error': 0.10908153795060657}\n",
            "Epoch 37/600\n",
            "13440/13440 [==============================] - 0s 30us/step - loss: 0.7118 - mean_squared_error: 0.1071 - val_loss: 0.7551 - val_mean_squared_error: 0.1292\n",
            "{'val_loss': 0.7551001787185669, 'val_mean_squared_error': 0.12919456760088602, 'loss': 0.7117926654361543, 'mean_squared_error': 0.10707941197213672}\n",
            "Epoch 38/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.7128 - mean_squared_error: 0.1075 - val_loss: 0.7537 - val_mean_squared_error: 0.1283\n",
            "{'val_loss': 0.7536647518475851, 'val_mean_squared_error': 0.1283269149561723, 'loss': 0.7128034932272774, 'mean_squared_error': 0.1074609134878431}\n",
            "Epoch 39/600\n",
            "13440/13440 [==============================] - 0s 30us/step - loss: 0.7094 - mean_squared_error: 0.1063 - val_loss: 0.7529 - val_mean_squared_error: 0.1283\n",
            "{'val_loss': 0.752943770090739, 'val_mean_squared_error': 0.1283409257729848, 'loss': 0.7094379175276984, 'mean_squared_error': 0.10629472498382841}\n",
            "Epoch 40/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.7094 - mean_squared_error: 0.1062 - val_loss: 0.7540 - val_mean_squared_error: 0.1286\n",
            "{'val_loss': 0.7540380954742432, 'val_mean_squared_error': 0.12855250065525373, 'loss': 0.7093871326673599, 'mean_squared_error': 0.10620895531915484}\n",
            "Epoch 41/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.7023 - mean_squared_error: 0.1046 - val_loss: 0.7174 - val_mean_squared_error: 0.1174\n",
            "{'val_loss': 0.7174150307973226, 'val_mean_squared_error': 0.1174129399160544, 'loss': 0.7023379274777004, 'mean_squared_error': 0.1046359828540257}\n",
            "Epoch 42/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.6989 - mean_squared_error: 0.1032 - val_loss: 0.7192 - val_mean_squared_error: 0.1179\n",
            "{'val_loss': 0.7191528836886089, 'val_mean_squared_error': 0.1178573692838351, 'loss': 0.6989010663259597, 'mean_squared_error': 0.10322379100890387}\n",
            "Epoch 43/600\n",
            "13440/13440 [==============================] - 0s 30us/step - loss: 0.6972 - mean_squared_error: 0.1031 - val_loss: 0.7016 - val_mean_squared_error: 0.1124\n",
            "{'val_loss': 0.7016498009363811, 'val_mean_squared_error': 0.11235455001393954, 'loss': 0.6972182881264459, 'mean_squared_error': 0.1031220929253669}\n",
            "Epoch 44/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.7000 - mean_squared_error: 0.1041 - val_loss: 0.7228 - val_mean_squared_error: 0.1185\n",
            "{'val_loss': 0.7228453834851583, 'val_mean_squared_error': 0.11849752987424532, 'loss': 0.7000449430374872, 'mean_squared_error': 0.1041326737120038}\n",
            "Epoch 45/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.6889 - mean_squared_error: 0.1007 - val_loss: 0.7458 - val_mean_squared_error: 0.1254\n",
            "{'val_loss': 0.7457614064216613, 'val_mean_squared_error': 0.12536687329411506, 'loss': 0.6888574730782282, 'mean_squared_error': 0.10068920425006321}\n",
            "Epoch 46/600\n",
            "13440/13440 [==============================] - 0s 30us/step - loss: 0.6884 - mean_squared_error: 0.1004 - val_loss: 0.7590 - val_mean_squared_error: 0.1294\n",
            "{'val_loss': 0.7589905182520549, 'val_mean_squared_error': 0.12943019221226373, 'loss': 0.68843256462188, 'mean_squared_error': 0.10039520221097128}\n",
            "Epoch 47/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.6873 - mean_squared_error: 0.0993 - val_loss: 0.7483 - val_mean_squared_error: 0.1265\n",
            "{'val_loss': 0.7483420451482137, 'val_mean_squared_error': 0.1265264036754767, 'loss': 0.6873077489080883, 'mean_squared_error': 0.09926791113047373}\n",
            "Epoch 48/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.6838 - mean_squared_error: 0.0989 - val_loss: 0.7474 - val_mean_squared_error: 0.1262\n",
            "{'val_loss': 0.747380793094635, 'val_mean_squared_error': 0.12617931937177976, 'loss': 0.6838129236584618, 'mean_squared_error': 0.0989285898350534}\n",
            "Epoch 49/600\n",
            "13440/13440 [==============================] - 0s 30us/step - loss: 0.6832 - mean_squared_error: 0.0991 - val_loss: 0.7428 - val_mean_squared_error: 0.1244\n",
            "{'val_loss': 0.7428315480550131, 'val_mean_squared_error': 0.12443228140473366, 'loss': 0.6832164866583688, 'mean_squared_error': 0.09914762846061162}\n",
            "Epoch 50/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.6768 - mean_squared_error: 0.0973 - val_loss: 0.7274 - val_mean_squared_error: 0.1198\n",
            "{'val_loss': 0.727393360932668, 'val_mean_squared_error': 0.11977703695495924, 'loss': 0.6767599372636705, 'mean_squared_error': 0.0973277231057485}\n",
            "Epoch 51/600\n",
            "13440/13440 [==============================] - 0s 30us/step - loss: 0.6822 - mean_squared_error: 0.0995 - val_loss: 0.7316 - val_mean_squared_error: 0.1209\n",
            "{'val_loss': 0.7315737962722778, 'val_mean_squared_error': 0.12086654553810755, 'loss': 0.6822315624782017, 'mean_squared_error': 0.09954222653593336}\n",
            "Epoch 52/600\n",
            "13440/13440 [==============================] - 0s 32us/step - loss: 0.6761 - mean_squared_error: 0.0967 - val_loss: 0.7557 - val_mean_squared_error: 0.1275\n",
            "{'val_loss': 0.7556992491086324, 'val_mean_squared_error': 0.1275350088874499, 'loss': 0.6760845962024871, 'mean_squared_error': 0.09665193564835049}\n",
            "Epoch 53/600\n",
            "13440/13440 [==============================] - 0s 30us/step - loss: 0.6771 - mean_squared_error: 0.0967 - val_loss: 0.7632 - val_mean_squared_error: 0.1290\n",
            "{'val_loss': 0.7631804704666137, 'val_mean_squared_error': 0.12901390145222347, 'loss': 0.6771400877407618, 'mean_squared_error': 0.0966854758205868}\n",
            "Epoch 54/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.6757 - mean_squared_error: 0.0969 - val_loss: 0.7540 - val_mean_squared_error: 0.1261\n",
            "{'val_loss': 0.7539738694826762, 'val_mean_squared_error': 0.12612895295023918, 'loss': 0.675709048906962, 'mean_squared_error': 0.09688110124497186}\n",
            "Epoch 55/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.6756 - mean_squared_error: 0.0971 - val_loss: 0.7537 - val_mean_squared_error: 0.1259\n",
            "{'val_loss': 0.7536896586418151, 'val_mean_squared_error': 0.12586689616243044, 'loss': 0.6756350647835504, 'mean_squared_error': 0.09709678228412355}\n",
            "Epoch 56/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.6747 - mean_squared_error: 0.0960 - val_loss: 0.7331 - val_mean_squared_error: 0.1198\n",
            "{'val_loss': 0.7331146717071533, 'val_mean_squared_error': 0.11975397889812787, 'loss': 0.674729725860414, 'mean_squared_error': 0.09600918548447746}\n",
            "Epoch 57/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.6705 - mean_squared_error: 0.0952 - val_loss: 0.7621 - val_mean_squared_error: 0.1278\n",
            "{'val_loss': 0.7620637853940327, 'val_mean_squared_error': 0.12778905828793843, 'loss': 0.6704805777186439, 'mean_squared_error': 0.09522852386747088}\n",
            "Epoch 58/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.6672 - mean_squared_error: 0.0943 - val_loss: 0.7828 - val_mean_squared_error: 0.1337\n",
            "{'val_loss': 0.7827923933664958, 'val_mean_squared_error': 0.13373145113388699, 'loss': 0.6671523656163897, 'mean_squared_error': 0.09434271405140558}\n",
            "Epoch 59/600\n",
            "13440/13440 [==============================] - 0s 30us/step - loss: 0.6694 - mean_squared_error: 0.0959 - val_loss: 0.7648 - val_mean_squared_error: 0.1287\n",
            "{'val_loss': 0.7648273428281148, 'val_mean_squared_error': 0.12868827109535536, 'loss': 0.6693998228935968, 'mean_squared_error': 0.09587817504292442}\n",
            "Epoch 60/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.6660 - mean_squared_error: 0.0941 - val_loss: 0.7463 - val_mean_squared_error: 0.1237\n",
            "{'val_loss': 0.7463151216506958, 'val_mean_squared_error': 0.12374992370605468, 'loss': 0.665987119220552, 'mean_squared_error': 0.09414131655579522}\n",
            "Epoch 61/600\n",
            "13440/13440 [==============================] - 0s 30us/step - loss: 0.6665 - mean_squared_error: 0.0944 - val_loss: 0.7306 - val_mean_squared_error: 0.1195\n",
            "{'val_loss': 0.7305835843086242, 'val_mean_squared_error': 0.11953545063734054, 'loss': 0.666481275785537, 'mean_squared_error': 0.09443585638489042}\n",
            "Epoch 62/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.6656 - mean_squared_error: 0.0942 - val_loss: 0.7655 - val_mean_squared_error: 0.1297\n",
            "{'val_loss': 0.765544331073761, 'val_mean_squared_error': 0.12974852149685223, 'loss': 0.6656016179493496, 'mean_squared_error': 0.09415731394574756}\n",
            "Epoch 63/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.6557 - mean_squared_error: 0.0923 - val_loss: 0.8028 - val_mean_squared_error: 0.1398\n",
            "{'val_loss': 0.8028369069099426, 'val_mean_squared_error': 0.13978046650687853, 'loss': 0.6556913035256522, 'mean_squared_error': 0.09226721099444798}\n",
            "Epoch 64/600\n",
            "13440/13440 [==============================] - 0s 30us/step - loss: 0.6666 - mean_squared_error: 0.0947 - val_loss: 0.7757 - val_mean_squared_error: 0.1328\n",
            "{'val_loss': 0.7757249236106872, 'val_mean_squared_error': 0.13277106036742528, 'loss': 0.6666352697781154, 'mean_squared_error': 0.09474760946773347}\n",
            "Epoch 65/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.6594 - mean_squared_error: 0.0924 - val_loss: 0.7552 - val_mean_squared_error: 0.1274\n",
            "{'val_loss': 0.7552138964335123, 'val_mean_squared_error': 0.12744481414556502, 'loss': 0.6593678678785052, 'mean_squared_error': 0.09242679803144364}\n",
            "Epoch 66/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.6565 - mean_squared_error: 0.0922 - val_loss: 0.7560 - val_mean_squared_error: 0.1278\n",
            "{'val_loss': 0.755986758073171, 'val_mean_squared_error': 0.1278162643313408, 'loss': 0.6565253246398199, 'mean_squared_error': 0.09216761127823875}\n",
            "Epoch 67/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.6612 - mean_squared_error: 0.0937 - val_loss: 0.7686 - val_mean_squared_error: 0.1313\n",
            "{'val_loss': 0.7685529748598735, 'val_mean_squared_error': 0.13127402886748313, 'loss': 0.6611972649892172, 'mean_squared_error': 0.0937392641391073}\n",
            "Epoch 68/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.6553 - mean_squared_error: 0.0916 - val_loss: 0.7663 - val_mean_squared_error: 0.1306\n",
            "{'val_loss': 0.7662715832392375, 'val_mean_squared_error': 0.1306214374800523, 'loss': 0.6553064312253679, 'mean_squared_error': 0.09161502690542311}\n",
            "Epoch 69/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.6525 - mean_squared_error: 0.0907 - val_loss: 0.7806 - val_mean_squared_error: 0.1343\n",
            "{'val_loss': 0.7806095321973164, 'val_mean_squared_error': 0.13433491463462513, 'loss': 0.652484256880624, 'mean_squared_error': 0.09073664297660192}\n",
            "Epoch 70/600\n",
            "13440/13440 [==============================] - 0s 30us/step - loss: 0.6479 - mean_squared_error: 0.0905 - val_loss: 0.7812 - val_mean_squared_error: 0.1341\n",
            "{'val_loss': 0.7811677018801372, 'val_mean_squared_error': 0.1340861571331819, 'loss': 0.6479378495897565, 'mean_squared_error': 0.09046965120803742}\n",
            "Epoch 71/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.6512 - mean_squared_error: 0.0908 - val_loss: 0.7542 - val_mean_squared_error: 0.1264\n",
            "{'val_loss': 0.7542227029800415, 'val_mean_squared_error': 0.12636871462066968, 'loss': 0.6511871530896142, 'mean_squared_error': 0.09083061729158674}\n",
            "Epoch 72/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.6507 - mean_squared_error: 0.0917 - val_loss: 0.7489 - val_mean_squared_error: 0.1249\n",
            "{'val_loss': 0.7489356915156047, 'val_mean_squared_error': 0.12487761005759239, 'loss': 0.6507287235487075, 'mean_squared_error': 0.09167063881953558}\n",
            "Epoch 73/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.6476 - mean_squared_error: 0.0900 - val_loss: 0.7573 - val_mean_squared_error: 0.1273\n",
            "{'val_loss': 0.7573169310887654, 'val_mean_squared_error': 0.12733829667170843, 'loss': 0.6476280660856337, 'mean_squared_error': 0.09004931151866913}\n",
            "Epoch 74/600\n",
            "13440/13440 [==============================] - 0s 30us/step - loss: 0.6485 - mean_squared_error: 0.0914 - val_loss: 0.7413 - val_mean_squared_error: 0.1231\n",
            "{'val_loss': 0.7412721753120423, 'val_mean_squared_error': 0.1230720912416776, 'loss': 0.6485466031801133, 'mean_squared_error': 0.091362188685508}\n",
            "Epoch 75/600\n",
            "13440/13440 [==============================] - 0s 30us/step - loss: 0.6466 - mean_squared_error: 0.0901 - val_loss: 0.7633 - val_mean_squared_error: 0.1293\n",
            "{'val_loss': 0.7632951140403748, 'val_mean_squared_error': 0.1292560135324796, 'loss': 0.6466132396743411, 'mean_squared_error': 0.09007593797785896}\n",
            "Epoch 76/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.6388 - mean_squared_error: 0.0880 - val_loss: 0.7741 - val_mean_squared_error: 0.1322\n",
            "{'val_loss': 0.7740719636281331, 'val_mean_squared_error': 0.13222091446320217, 'loss': 0.6388331208910261, 'mean_squared_error': 0.08802724395479475}\n",
            "Epoch 77/600\n",
            "13440/13440 [==============================] - 0s 31us/step - loss: 0.6469 - mean_squared_error: 0.0906 - val_loss: 0.7810 - val_mean_squared_error: 0.1345\n",
            "{'val_loss': 0.781026303768158, 'val_mean_squared_error': 0.13448745235800744, 'loss': 0.6469176264036269, 'mean_squared_error': 0.09058151883738381}\n",
            "Epoch 78/600\n",
            "13440/13440 [==============================] - 0s 29us/step - loss: 0.6451 - mean_squared_error: 0.0904 - val_loss: 0.7901 - val_mean_squared_error: 0.1373\n",
            "{'val_loss': 0.7900862296422323, 'val_mean_squared_error': 0.13729164799054464, 'loss': 0.6451284641311282, 'mean_squared_error': 0.09037956708953494}\n",
            "Epoch 79/600\n",
            "13440/13440 [==============================] - 0s 30us/step - loss: 0.6417 - mean_squared_error: 0.0888 - val_loss: 0.7934 - val_mean_squared_error: 0.1385\n",
            "{'val_loss': 0.7934019565582275, 'val_mean_squared_error': 0.13852415730555853, 'loss': 0.6417273634955997, 'mean_squared_error': 0.08876889936980747}\n",
            "Epoch 80/600\n",
            "13440/13440 [==============================] - 0s 30us/step - loss: 0.6403 - mean_squared_error: 0.0896 - val_loss: 0.7995 - val_mean_squared_error: 0.1407\n",
            "{'val_loss': 0.7995176593462626, 'val_mean_squared_error': 0.14070978462696077, 'loss': 0.6402735256013417, 'mean_squared_error': 0.08958505448840913}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}